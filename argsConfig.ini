[Model]
dropout_input = 0.1
# Small == 256, Large == 1024
feature_num = 256
visualize_model_graph = True

[Data]
# only for server
;dataset = /home/space/datasets/ag_news
#/home/space/datasets/yelp/

dataset = datasets/ag_news
#yelp
chunk_size = 1000
encoding = utf-8
max_csv_rows = 10000
csv_sep = ,
balance_classes = True
ratio = 1
preprocess_data = True
steps = None
#steps = remove_hashtags,remove_urls,remove_user_mentions,lower
usecols = 0,1,2
#0,1,2 for ag news
#0,1 for yelp
# kFoldCrossValidation is applied if k_folds > 1
k_folds = 5

[DataSet]
alphabet = abcdefghijklmnopqrstuvwxyz0123456789
    -,;.!?:'"/\|_@#$%%^&*~`+-=<>()[]{}
char_num = 70
l0 = 1014

[Train]
batch_size = 128
train_size = 0.85
dev_size = 0.15
use_sampler = True
workers = 6
criterion = nllloss
#Adam, SGD, ASGD
optimizer = SGD
# step, clr
scheduler = step
clr_step_size = 4
clr_min_lr = 0.0017
clr_max_lr = 0.01
epochs = 1
max_norm = 400
lr = 0.01
continue_from_checkpoint = False
dynamic_lr = True
milestones = 10,15,20
decay_factor = 0.5
early_stopping = False
patience =
# print train status every (epoch % print_out_every == 0)
print_out_every = 1

[Test]
model_log_dir = None
model_to_test = ./models/yelp/model_yelpModel_epoch_5_l0_1014_lr_0.005_loss_0.1852_acc_0.9277_f1_0.9276.pt


[Log]
# You need to be careful we using the following options
# some options activated together will give you headache
flush_history = True
# for server only
;log_path = /home/pml_28/MS1/logs/
;output = /h/home/pml_28/MS1/logs/

log_path = logs/
output = models/

log_f1 = True
checkpoint = True
# Save model even if not best every (epoch % save_interval == 0)
save_interval = 0
model_name =
# print train status every (epoch % print_out_every == 0)
print_out_every = 1
# Path to model checkpoint to continue training from there
# If you don't want this option put the value None
continue_from_model_checkpoint = None
delete_model_name_dir = False

[Device]
enable_gpu = False
